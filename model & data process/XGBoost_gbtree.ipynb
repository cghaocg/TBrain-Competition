{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "from time import time, strftime, localtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"../dataset/test.csv\")\n",
    "submit_test_df = pd.read_csv(\"../dataset/submit_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data & process missing value\n",
    "\n",
    "Fill nan with:\n",
    "* \"0.0\"\n",
    "* median()\n",
    "* mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nan with \"0.0\"\n",
    "train_fillna_df = train_df.iloc[:,1:-1].fillna(value=0.0)\n",
    "test_fillna_df = test_df.iloc[:,1:].fillna(value=0.0)\n",
    "all_fillna_df = pd.concat((train_fillna_df, test_fillna_df), axis = 0)\n",
    "\n",
    "# Fill nan with \"median\"\n",
    "#train_fillna_df = train_df.iloc[:,1:-1].fillna(train_df.iloc[:,1:-1].median())\n",
    "#test_fillna_df = test_df.iloc[:,1:].fillna(test_df.iloc[:,1:].median())\n",
    "#all_fillna_df = pd.concat((train_fillna_df, test_fillna_df), axis = 0)\n",
    "\n",
    "# Seperate features fillna\n",
    "#train_noIDnoLabel_df = train_df.iloc[:,1:-1]\n",
    "#test_noID_df = test_df.iloc[:,1:]\n",
    "#all_df = pd.concat((train_noIDnoLabel_df, test_noID_df), axis = 0)\n",
    "\n",
    "label_name = \"total_price\"\n",
    "label_df = pd.DataFrame(train_df[label_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_df = all_fillna_df.copy()\n",
    "temp_df = all_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process miss value\n",
    "\n",
    "features:\n",
    "* village_income_median 所在里年收入中位數：以 \"city+town\" 為 key 填入平均值，如無值則以 \"city\" 為 key 填入平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\me\\Anaconda3\\envs\\tensorflow-gpu-py3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spent time: 63.623s 2019-07-12 17:59:29\n",
      "count: 1326\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "VIM_mean = 0.0\n",
    "t0 = time()\n",
    "\n",
    "# Fill nan with the mean of \"city+town\"\n",
    "for index, row in all_df.iloc[:][(all_df[\"village_income_median\"].isnull())].iterrows():\n",
    "    count += 1\n",
    "    VIM_mean = all_df[\"village_income_median\"][(all_df[\"city\"] == row[\"city\"]) & (all_df[\"town\"] == row[\"town\"])].mean()\n",
    "    # If the mean of \"city+town\" is null or zero, fill nan with the mean of \"city\"\n",
    "    if math.isnan(VIM_mean) or VIM_mean == 0:\n",
    "        VIM_mean = all_df[\"village_income_median\"][all_df[\"city\"] == row[\"city\"]].mean()\n",
    "    temp_df[\"village_income_median\"][(temp_df[\"village_income_median\"].isnull()) & (temp_df[\"city\"] == row[\"city\"]) & (temp_df[\"town\"] == row[\"town\"])] = VIM_mean\n",
    "\n",
    "print('spent time: %0.3fs' % (time() - t0), strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "print (\"count:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nan with \"0.0\"\n",
    "all_fillna_df = temp_df.fillna(value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop no need features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_process_df.shape: (70000, 231)\n"
     ]
    }
   ],
   "source": [
    "all_process_df = all_fillna_df.drop(\n",
    "    [\"parking_area\",\"parking_price\"], axis=1)\n",
    "print(\"all_process_df.shape:\", all_process_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features\n",
    "* [house_age 購買時的屋齡]：以10年為一個區間，((成交日 - 完工日) / 3650) ((txn_dt - building_complete_dt) / 3650)\n",
    "* [txn_year 交易年]：the year of trade，成交日 / 365, txn_dt / 365\n",
    "* [floor_of_building 樓層高度比例]：floor of building，交易樓層 / 總樓層(txn_floor / total_floor)，如 txn_floor 為 0，代表建物不為一層，以總樓層帶入\n",
    "* [sales_per_unit_area 坪效]：一坪土地可蓋出多少坪可銷售面積，(建物面積 / 土地面積)(building_area / land_area)\n",
    "* [power_pos_rate 強化正相關率]：將會提高房價的因素加總，再減會減低房價的因素，(所在縣市出生率 + 所在縣市結婚率 - 所在縣市死亡率) (born_rate + marriage_rate - death_rate)\n",
    "* [high_edu_rate 各縣市高學歷率]：各縣市的高學歷加總比率(大學含以上)\n",
    "* [low_edu_rate 各縣市低學歷率]：各縣市的低學歷加總比率(專科+高中含以下)\n",
    "* [is_apartment_first_top 是否為公寓的一樓跟頂樓]：公寓的一樓跟頂樓最貴\n",
    "* [is_0_3_high_floor 是否為電梯大樓、華廈的高樓層(7樓以上)]：樓層越高越貴\n",
    "* [_0_3_high_floor_rate 電梯大樓、華廈的高樓層高度比率(7樓以上)]：樓層越高越貴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\me\\Anaconda3\\envs\\tensorflow-gpu-py3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# [house_age: the age of house on purchased date, ((成交日 - 完工日) / 3650) ((txn_dt - building_complete_dt) / 3650)]\n",
    "all_createCol_df = all_fillna_df.assign(house_age = round((all_fillna_df[\"txn_dt\"] - all_fillna_df[\"building_complete_dt\"]) / 3650, 0))\n",
    "\n",
    "# [txn_year: the year of trade, 成交日 / 365, txn_dt / 365]\n",
    "all_createCol_df = all_createCol_df.assign(txn_year = round((all_fillna_df[\"txn_dt\"] / 365), 0))\n",
    "\n",
    "# [floor_of_building: floor of building, 交易樓層 / 總樓層(txn_floor / total_floor)]，如 txn_floor 為 0，代表建物不為一層，以總樓層帶入\n",
    "all_createCol_df = all_createCol_df.assign(floor_of_building = all_fillna_df[\"txn_floor\"] / all_fillna_df[\"total_floor\"])\n",
    "all_createCol_df[\"floor_of_building\"][all_createCol_df[\"txn_floor\"] == 0] = all_fillna_df[\"total_floor\"]\n",
    "\n",
    "inf = float(\"inf\")\n",
    "# [sales_per_unit_area 坪效: 一坪土地可蓋出多少坪可銷售面積, (建物面積 / 土地面積), (building_area / land_area)]\n",
    "all_createCol_df = all_createCol_df.assign(sales_per_unit_area = round(all_createCol_df[\"building_area\"] / all_createCol_df[\"land_area\"], 1))\n",
    "all_createCol_df[\"sales_per_unit_area\"][all_createCol_df[\"sales_per_unit_area\"] == inf] = all_createCol_df[\"sales_per_unit_area\"].median()\n",
    "\n",
    "# [power_pos_rate 強化正相關率: 將會提高房價的因素加總，再減會減低房價的因素, (所在縣市出生率 + 所在縣市結婚率 - 所在縣市死亡率), (born_rate + marriage_rate - death_rate)]\n",
    "all_createCol_df = all_createCol_df.assign(power_pos_rate = all_createCol_df[\"born_rate\"] + all_createCol_df[\"marriage_rate\"] - all_createCol_df[\"death_rate\"])\n",
    "#all_createCol_df = all_fillna_df.assign(power_pos_rate = all_fillna_df[\"born_rate\"] + all_fillna_df[\"marriage_rate\"] - all_fillna_df[\"death_rate\"])\n",
    "\n",
    "# [high_edu_rate 各縣市高學歷率: 各縣市的高學歷加總比率(大學含以上)]\n",
    "#all_createCol_df = all_createCol_df.assign(high_edu_rate = round(all_createCol_df[\"doc_rate\"] + all_createCol_df[\"master_rate\"] + all_createCol_df[\"bachelor_rate\"], 3))\n",
    "\n",
    "# [low_edu_rate 各縣市低學歷率: 各縣市的低學歷加總比率(專科+高中含以下)]\n",
    "#all_createCol_df = all_createCol_df.assign(low_edu_rate = round(all_createCol_df[\"jobschool_rate\"] + all_createCol_df[\"highschool_rate\"] + all_createCol_df[\"junior_rate\"] + all_createCol_df[\"elementary_rate\"], 3))\n",
    "\n",
    "# [is_apartment_first_top 是否為公寓的一樓跟頂樓: 公寓的一樓跟頂樓最貴]\n",
    "all_createCol_df = all_createCol_df.assign(is_apartment_first_top = 0.0)\n",
    "all_createCol_df[\"is_apartment_first_top\"][(all_createCol_df[\"building_type\"] == 1) & ((all_createCol_df[\"txn_floor\"] == 1) |  (all_createCol_df[\"txn_floor\"] == all_createCol_df[\"total_floor\"]))] = 1\n",
    "\n",
    "# [is_0_3_high_floor 是否為電梯大樓、華廈的高樓層(7樓以上): 樓層越高越貴]\n",
    "#all_createCol_df = all_createCol_df.assign(is_0_3_high_floor = 0.0)\n",
    "#all_createCol_df[\"is_0_3_high_floor\"][((all_createCol_df[\"building_type\"] == 0) | (all_createCol_df[\"building_type\"] == 3)) & (all_createCol_df[\"txn_floor\"] >= 7)] = 1\n",
    "\n",
    "# [_0_3_high_floor_rate 電梯大樓、華廈的高樓層高度比率(7樓以上): 樓層越高越貴]\n",
    "#all_createCol_df = all_createCol_df.assign(_0_3_high_floor_rate = 0.0)\n",
    "#all_createCol_df[\"_0_3_high_floor_rate\"][((all_createCol_df[\"building_type\"] == 0) | (all_createCol_df[\"building_type\"] == 3)) & (all_createCol_df[\"txn_floor\"] >= 7)] = (all_fillna_df[\"txn_floor\"] / all_fillna_df[\"total_floor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_apartment_first_top</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_apartment_first_top\n",
       "0                     0.0\n",
       "1                     1.0\n",
       "2                     1.0\n",
       "3                     0.0\n",
       "4                     0.0"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_createCol_df[[\"is_apartment_first_top\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 239)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_createCol_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_process_df = all_df.copy()\n",
    "#all_process_df = all_selection_df.copy()\n",
    "#all_process_df = all_fillna_df.copy()\n",
    "all_process_df = all_createCol_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "* min max scalar: normalize with training data & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mms_train_process_df.shape: (60000, 239)\n",
      "mms_test_process_df.shape: (10000, 239)\n"
     ]
    }
   ],
   "source": [
    "# [No-ohe version]\n",
    "# preserve column name after scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "mms_features_df = pd.DataFrame(min_max_scaler.fit_transform(all_process_df))\n",
    "\n",
    "## Split to train & test data\n",
    "train_num = train_df.shape[0]\n",
    "#train_num = train_remove_outlier_df.shape[0]\n",
    "mms_train_process_df = mms_features_df.iloc[:train_num,:]\n",
    "mms_test_process_df = mms_features_df.iloc[train_num:,:]\n",
    "\n",
    "print(\"mms_train_process_df.shape:\", mms_train_process_df.shape)\n",
    "print(\"mms_test_process_df.shape:\", mms_test_process_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle data & Split data\n",
    "* random_state = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (51000, 239)\n",
      "y_train.shape: (51000, 1)\n",
      "X_val.shape: (9000, 239)\n",
      "y_val.shape: (9000, 1)\n"
     ]
    }
   ],
   "source": [
    "#shuff_train_all = shuffle(pd.concat((mms_train_process_df, label_df), axis=1))\n",
    "# No min max scalar\n",
    "#shuff_train_all = shuffle(pd.concat((train_process_df, label_df), axis=1))\n",
    "# Drop unimportant features\n",
    "#shuff_train_all = shuffle(pd.concat((temp_train_df, label_df), axis=1))\n",
    "# min max\n",
    "shuff_train_all = shuffle(pd.concat((mms_train_process_df, label_df), axis=1), random_state=7)\n",
    "#shuff_train_all = shuffle(pd.concat((mms_train_process_df, label_df), axis=1))\n",
    "# Drop & min max & remove outliers\n",
    "#shuff_train_all = shuffle(pd.concat((mms_train_process_df, label_remove_outliers_df), axis=1), random_state=0)\n",
    "# Drop & remove outliers\n",
    "#shuff_train_all = shuffle(pd.concat((train_drop_df, label_remove_outliers_df), axis=1), random_state=0)\n",
    "\n",
    "\n",
    "# process train and validation num\n",
    "tv_num = round(shuff_train_all.shape[0] * 0.85)\n",
    "\n",
    "### Split data\n",
    "# training data\n",
    "X_train = shuff_train_all.iloc[:tv_num,:-1]\n",
    "y_train = shuff_train_all.iloc[:tv_num,-1:]\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "\n",
    "# validation data\n",
    "X_val = shuff_train_all.iloc[tv_num:,:-1]\n",
    "y_val = shuff_train_all.iloc[tv_num:,-1:]\n",
    "\n",
    "print(\"X_val.shape:\", X_val.shape)\n",
    "print(\"y_val.shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfter into data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalidation = xgb.DMatrix(X_val, label=y_val)\n",
    "#dtest = xgb.DMatrix(mms_test_process_df)\n",
    "# No min max scaler\n",
    "#dtest = xgb.DMatrix(test_process_df)\n",
    "# Drop unimportant features\n",
    "#dtest = xgb.DMatrix(temp_test_df)\n",
    "# Drop & min max\n",
    "dtest = xgb.DMatrix(mms_test_process_df)\n",
    "# Drop\n",
    "#dtest = xgb.DMatrix(test_drop_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "* XGBoost\n",
    "    * booster: gradient boosting tree\n",
    "    * objective: regression with squared loss\n",
    "    * eval_metric: RMSE\n",
    "    * learning rate: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'objective': 'reg:squarederror', 'colsample_bytree': 0.8, 'subsample': 0.5, 'max_depth': 10, 'eta': 0.01, \n",
    "        'min_child_weight': 0.3, 'reg_alpha': 0.01, 'reg_lambda': 1, 'gamma': 0.0001, 'verbosity': 1, 'random_state': 7, \n",
    "        'nthread': -1, 'tree_method': 'gpu_hist', 'booster': 'gbtree'}\n",
    "param['eval_metric'] = ['rmse']\n",
    "\n",
    "### Training model\n",
    "print('開始時間：', strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "t0 = time()\n",
    "\n",
    "#evallist = [(dtrain, 'train'), (dvalidation, 'eval')]\n",
    "evallist = [(dvalidation, 'eval'), (dtrain, 'train')]\n",
    "num_round = 200000\n",
    "#bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "bst = xgb.train(param, dtrain, num_round, evallist, early_stopping_rounds=10)\n",
    "\n",
    "print('spent time: %0.3fs' % (time() - t0), strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "\n",
    "### Predict\n",
    "print('開始時間：', strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "t0 = time()\n",
    "\n",
    "xgb_pred = bst.predict(dtest)\n",
    "y_test = bst.predict(dtrain)\n",
    "\n",
    "print('spent time: %0.3fs' % (time() - t0), strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "\n",
    "# validation\n",
    "RMSE = np.sqrt(mean_squared_error(y_train, y_test))\n",
    "print(\"{:,}\".format(RMSE.round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "* RMSE on training data\n",
    "* RMSE on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_train = np.sqrt(mean_squared_error(y_train, y_test))\n",
    "print('RMSE_train:', \"{:,}\".format(RMSE_train.round(4)))\n",
    "\n",
    "y_val_test = bst.predict(dvalidation)\n",
    "RMSE_val = np.sqrt(mean_squared_error(y_val, y_val_test))\n",
    "print('RMSE_val:', \"{:,}\".format(RMSE_val.round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:         total_price\n",
      "count  5.100000e+04\n",
      "mean   1.284103e+07\n",
      "std    5.108065e+07\n",
      "min    2.261495e+05\n",
      "25%    2.436585e+06\n",
      "50%    5.240482e+06\n",
      "75%    1.123932e+07\n",
      "max    4.357039e+09\n",
      "y_test:                   0\n",
      "count  5.100000e+04\n",
      "mean   1.284045e+07\n",
      "std    5.101806e+07\n",
      "min   -5.353824e+06\n",
      "25%    2.635457e+06\n",
      "50%    5.391494e+06\n",
      "75%    1.117924e+07\n",
      "max    4.355997e+09\n",
      "xgb_pred:                   0\n",
      "count  1.000000e+04\n",
      "mean   1.319855e+07\n",
      "std    4.696391e+07\n",
      "min   -2.750140e+07\n",
      "25%    2.580645e+06\n",
      "50%    5.396035e+06\n",
      "75%    1.103688e+07\n",
      "max    2.038183e+09 \n",
      "\n",
      "number of xgb_pred are negative: 36\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train:\", pd.DataFrame(y_train).describe())\n",
    "print(\"y_test:\", pd.DataFrame(y_test).describe())\n",
    "print(\"xgb_pred:\", pd.DataFrame(xgb_pred).describe(), \"\\n\")\n",
    "print(\"number of xgb_pred are negative:\", len(xgb_pred[xgb_pred < 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model/xgb_tree_1.pkl', 'wb') as f:\n",
    "    pickle.dump(bst, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "自動產出數字最新的 submit 檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filePath: ../submit/submit_test_72.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# 取得 submit 檔最大值的下一個數字\n",
    "def getMaxFileNum():\n",
    "    max = 0\n",
    "    for f in os.listdir(\"../submit\"):\n",
    "        fileNum = re.findall('\\d+', f)\n",
    "        if len(fileNum):\n",
    "            if int(fileNum[0]) > max:\n",
    "                max = int(fileNum[0])\n",
    "    return str(max + 1)\n",
    "\n",
    "filePath = \"../submit/submit_test_\" + getMaxFileNum() + \".csv\"\n",
    "\n",
    "pred_df = pd.DataFrame(np.array(xgb_pred), columns=[\"total_price\"])\n",
    "ans_df = pd.merge(submit_test_df[\"building_id\"].to_frame(), pred_df, left_index=True, right_index=True, how=\"outer\")\n",
    "ans_df.to_csv(filePath,sep=\",\",index=False,encoding=\"UTF-8\")\n",
    "\n",
    "print('filePath:', filePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
