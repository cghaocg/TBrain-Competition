{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data overview\n",
    "* training data\n",
    "    * row num: 60,000\n",
    "    * column num: 233(not including ID and ground truth)\n",
    "    * missing value:\n",
    "\n",
    "Column Name              | Column Index  | Number of Missing Value | Percentage of Missing Value\n",
    "------------------|:------------:|:-------------------:|:----------------------:\n",
    "parking_area               |         9            |               56,897            |                 94.83%\n",
    "parking_price              |         10           |               46,065            |                 76.78%\n",
    "txn_floor                     |         11           |                15,902            |                 26.50% \n",
    "village_income_median |         17           |                  1,142            |                    1.90%\n",
    "\n",
    "\n",
    "* testing data\n",
    "    * row num: 10,000\n",
    "    * column num: 233(not including ID)\n",
    "    * missing value:\n",
    "    \n",
    "Column Name              | Column Index  | Number of Missing Value | Percentage of Missing Value\n",
    "------------------|:------------:|:-------------------:|:----------------------:\n",
    "parking_area               |         9            |               9,500             |                 95.00%\n",
    "parking_price              |         10           |               7,710              |                 77.10%\n",
    "txn_floor                     |         11           |               2,639             |                 26.39% \n",
    "village_income_median |         17           |                  184             |                    1.84%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data_df = pd.read_csv(\"../dataset/train.csv\")\n",
    "test_data_df = pd.read_csv(\"../dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column name of missing value(training data):, parking_area, parking_price, txn_floor, village_income_median\n",
      "The column index of missing value(training data):, 9, 10, 11, 17\n",
      "The number of missing value(training data):, 56897, 46065, 15902, 1142\n",
      "The percentage of missing value(training data): 94.83% 76.78% 26.50% 1.90%\n",
      "\n",
      "The column name of missing value(testing data):, parking_area, parking_price, txn_floor, village_income_median\n",
      "The column index of missing value(testing data):, 9, 10, 11, 17\n",
      "The number of missing value(testing data):, 9500, 7710, 2639, 184\n",
      "The percentage of missing value(testing data): 95.00% 77.10% 26.39% 1.84%\n"
     ]
    }
   ],
   "source": [
    "train_isnull_ser = train_data_df.isnull().sum(axis=0)\n",
    "train_data_num = train_data_df.shape[0]\n",
    "print(\"The column name of missing value(training data):\", *[temp for temp in train_data_df.columns[train_isnull_ser.nonzero()]], sep = \", \")\n",
    "print(\"The column index of missing value(training data):\", *[temp for temp in train_isnull_ser.nonzero()[0]], sep = \", \")\n",
    "print(\"The number of missing value(training data):\", *[train_isnull_ser[i] for i in train_isnull_ser.nonzero()[0]], sep = \", \")\n",
    "print(\"The percentage of missing value(training data):\", *[\"{0:.2%}\".format(train_isnull_ser[i] / train_data_num) for i in train_isnull_ser.nonzero()[0]])\n",
    "print()\n",
    "test_isnull_ser = test_data_df.isnull().sum(axis=0)\n",
    "test_data_num = test_data_df.shape[0]\n",
    "print(\"The column name of missing value(testing data):\", *[temp for temp in test_data_df.columns[test_isnull_ser.nonzero()]], sep = \", \")\n",
    "print(\"The column index of missing value(testing data):\", *[temp for temp in test_isnull_ser.nonzero()[0]], sep = \", \")\n",
    "print(\"The number of missing value(testing data):\", *[test_isnull_ser[i] for i in test_isnull_ser.nonzero()[0]], sep = \", \")\n",
    "print(\"The percentage of missing value(testing data):\", *[\"{0:.2%}\".format(test_isnull_ser[i] / test_data_num) for i in test_isnull_ser.nonzero()[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. data process & model & method\n",
    "## data process\n",
    "1. missing value\n",
    "    1. 帶0、帶mean()、帶median()\n",
    "    2. village_income_median(所在里年收入中位數)\n",
    "        1. 原先想以 city+town+village 為 key 值找出同 key 的平均數填入，但發現同 key 的地方值都是 null\n",
    "        2. 改以 city+town 為 key 值找出同 key 的平均數填入，如此 key 為空或 0，改以 city 為 key 值找出同 key 的平均數填入\n",
    "2. min max scaler\n",
    "3. one hot encoding\n",
    "4. 移除重要性低的 feature (xgboost.plot_importance)\n",
    "5. 移除 outliers(以眼睛判斷)，約移除 1W 筆\n",
    "6. 移除資料分布差異過大、種類過多的 feature，移除 6 筆\n",
    "7. 使用 sklearn.feature_selection.SelectPercentile(f_regression, percentile=95)\n",
    "    * 重要 features\n",
    "        * **txn_dt：有無此特徵差1千多分**\n",
    "8. Create new features\n",
    "    1. house_age\n",
    "        * 購買當下的屋齡，以10年為單位(四捨五入)，((成交日 - 完工日) / 3650) ((txn_dt - building_complete_dt) / 3650)\n",
    "    2. txn_year\n",
    "        * 成交年，以年為單位(四捨五入)，(成交日 / 365) (txn_dt / 365)\n",
    "    3. floor_of_building\n",
    "        * 交易樓層於總樓層的位置，交易樓層/總樓層(txn_floor/total_floor)\n",
    "        * 如txn_floor為0，代表交易建物不為一層，此值帶入total_floor\n",
    "    4. sales_per_unit_area 坪效\n",
    "        * 一坪土地可蓋出多少坪可銷售面積(四捨五入至小數一位)，(建物面積 / 土地面積) (building_area / land_area)\n",
    "        * missing value 補中位數\n",
    "    5. power_pos_rate 強化正相關率\n",
    "        * 將會提高房價的因素加總，再減會減低房價的因素，(所在縣市出生率 + 所在縣市結婚率 - 所在縣市死亡率) (born_rate + marriage_rate - death_rate)\n",
    "    6. high_edu_rate 各縣市高學歷率\n",
    "        * 各縣市的高學歷加總比率(大學含以上)，(博士比例 + 碩士比例 +  學士比例) (doc_rate + master_rate + bachelor_rate)\n",
    "    7. low_edu_rate 各縣市低學歷率(考慮刪除)\n",
    "        * 各縣市的低學歷加總比率(專科+高中含以下)，(專科比例 + 高中比例 +  國中比例 + 小學比例) (jobschool_rate + highschool_rate + junior_rate + elementary_rate)\n",
    "    8. is_apartment_first_top 是否為公寓的一樓跟頂樓\n",
    "        * 公寓的一樓跟頂樓最貴，判斷建物型態(building_type)為 1 (公寓)的建物，如交易樓層為一樓、頂樓，則設值為 1\n",
    "    9. is_0_3_high_floor 是否為電梯大樓、華廈的高樓層\n",
    "        * 電梯大樓、華廈的樓層越高越貴，判斷建物型態(building_type)為 0 (電梯大樓)、3 (華廈)的建物，如交易樓層大於等於 7 樓，則設值為 1\n",
    "    10. \\_0_3_high_floor_rate 電梯大樓、華廈的高樓層高度比率(7樓以上)\n",
    "        * 電梯大樓、華廈的樓層越高越貴，判斷建物型態(building_type)為 0 (電梯大樓)、3 (華廈)的建物，如交易樓層大於等於 7 樓，則設樓層高度比率 \n",
    "        * (交易樓層 / 總樓層) (txn_floor / total_floor)\n",
    "        \n",
    "## model result\n",
    "* 模型參數皆記錄於\"Submission History\"\n",
    "* xgboost (gbtree) + data process(1.,2.) + **data process(3.)**：分數很低，因為此 algo 本質就是DTree，所以全 feature 做 one hot 只是徒增 feature 數\n",
    "* xgboost (gbtree) + data process(1.,2.) + **data process(4.)**：分數普通，約3~4k\n",
    "* xgboost (gbtree) + data process(1.,2.) + **data process(5.)**：分數很低，約1~2k\n",
    "* xgboost (gbtree) + data process(1.,2.) + **data process(6.)**：分數普通，約3~4k\n",
    "* xgboost (gbtree) + data process(1.,2.) + **data process(7.)**：分數普通，約5k\n",
    "* xgboost (gbtree) + data process(1-A.,2.)：分數普通，約5.3k\n",
    "* xgboost (gbtree) + data process(1-A.,1-B.,2.) + **data process(8-A.)**：分數普通，約5.2k\n",
    "* xgboost (gbtree) + data process(1-A.,1-B.,2.) + **data process(8-A.,8-C.)**：分數普通，約5.1k\n",
    "* xgboost (gbtree) + data process(1-A.,2.) + **data process(8-A.~8-C.)**：分數普通，約5k\n",
    "* xgboost (gbtree) + data process(1-A.,2.) + **data process(8-A.~8-G.)**：分數普通，約5.3k\n",
    "* xgboost (gbtree) + data process(1-A.,2.) + **data process(8-A.~8-D., 8-H.)**：分數普通，約5.3k\n",
    "* xgboost (gbtree) + data process(1-A.,2.) + **data process(8-A.~8-C., 8-E., 8-H., 8-J.)**：分數普通，約5.2k\n",
    "\n",
    "* xgboost (gblinear)：suck\n",
    "* sklearn.linear_model.Lasso：初步結果不佳，尚在做不同 data process & tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. 疑惑點\n",
    "* 模型 xgboost (gbtree)：比較上傳平台的真實分數，對 train loss 做 early stopping 往往比對 eval loss 做分數要高一千多分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. 獲得經驗\n",
    "* 模型 xgboost (gbtree)：目前不管怎麼處理 feature，上傳分數最高者仍是不處理\n",
    "* 模型 xgboost (gbtree)：移除 training data outliers (以眼睛判別，移除過2w、8k筆)、其後分別對 training data、testing data 做 min max scaler，雖 eval loss 比較低，但實際上傳分數極低\n",
    "* 模型 xgboost (gbtree)：使用套件 GridSearchCV 去選擇參數比較方便 (目前僅用 training data 作評分準則)\n",
    "* 資料前處理：missing value 全 feature 以 mean 或 median 填入，training、testing分開填入、合併填入，分數都跟5300版本沒太大差異(模型為 xgboost (gbtree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. 心得\n",
    "* 每次 shuffle data 後，eval 的 loss 值會有誤差，shuffle 時加上 random_state 維持一貫性，才好判別模型優劣\n",
    "* 模型 xgboost (gbtree)：train 到 train loss 值停止下降時，通常上傳分數較好\n",
    "* Data normalization：training data + testing data 一起做 min max scaler 會比兩者分開做的上傳分數好很多，儘管 train or eval loss 比較好\n",
    "* Data normalization：Decision Trees 的特性不需要做 feature rescaling (ex: min max scaler)，如做則必須將 training data 與 testing data 一起做，否則預測值會完全失真"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
